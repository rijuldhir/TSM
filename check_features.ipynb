{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.load('features.npy')\n",
    "lis = []\n",
    "for i,j in enumerate(x):\n",
    "    lis.append((i,j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lis[0])\n",
    "new_lis = []\n",
    "\n",
    "for x in lis:\n",
    "    new_lis.append(np.mean(x[1],axis=0))\n",
    "print(new_lis[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib\n",
    "import json\n",
    "\n",
    "linked = linkage(np.asarray(new_lis), 'single')\n",
    "with open('../dataset/something-something-v2-labels.json') as f:\n",
    "    reads = f.read()\n",
    "    x = json.loads(reads)\n",
    "rev_id = {}\n",
    "for a,b in x.items():\n",
    "    rev_id[b] = a\n",
    "labelList = [a for a,b in x.items()]\n",
    "\n",
    "matplotlib.rc('xtick', labelsize=200) \n",
    "matplotlib.rc('ytick', labelsize=20)\n",
    "ax = plt.figure(figsize=(60, 40))\n",
    "#ax = plt.subplots()\n",
    "#ax.tick_params(axis='both', which='major', labelsize=10) \n",
    "#ax.tick_params(axis='both', which='minor', labelsize=8)\n",
    "dendrogram(linked,\n",
    "            orientation='top',\n",
    "            labels=labelList,\n",
    "            distance_sort='descending',\n",
    "            show_leaf_counts=True)\n",
    "fig = plt.gcf()\n",
    "plt.savefig('foo.png', dpi=600)\n",
    "#plt.show()\n",
    "#plt.savefig('clusters.png')\n",
    "#plt.savefig('foo.png', bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from collections import defaultdict\n",
    "\n",
    "cluster = AgglomerativeClustering(n_clusters=48, affinity='euclidean', linkage='ward')\n",
    "cluster.fit_predict(np.asarray(new_lis))\n",
    "print(cluster.labels_)\n",
    "\n",
    "dic = defaultdict(list)\n",
    "print(dic)\n",
    "#print(rev_id)\n",
    "for i,k in enumerate(cluster.labels_):\n",
    "    dic[int(k)].append(rev_id[str(i)])\n",
    "for x,y in dic.items():\n",
    "    print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "def cos_cdist(vector):\n",
    "    # getting cosine distance between search image and images database\n",
    "    v = vector.reshape(1, -1)\n",
    "    return scipy.spatial.distance.cdist(new_lis, v, 'cosine').reshape(-1)\n",
    "\n",
    "for i,f in enumerate(new_lis):\n",
    "    img_distances = cos_cdist(f)\n",
    "    nearest_ids = np.argsort(img_distances)[:5].tolist()\n",
    "    #print(rev_id[str(i)])\n",
    "    lis = [rev_id[str(x)] for x in nearest_ids]\n",
    "    print(lis)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import deepdish as dd\n",
    "abc = np.asarray([1,2,3],dtype=np.float32)\n",
    "dic = {'14324':[1,[[1],2,3,4]],'1212':[2,[1,2]],'11':abc}\n",
    "s = str(dic)\n",
    "print(s)\n",
    "dd.io.save('test.h5', {'data': dic}, compression=None)\n",
    "\n",
    "#with h5py.File('feat.hdf5','w') as f:\n",
    "#    f.create_dataset(s,dtype=\"f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''import ast \n",
    "h = h5py.File('feat.hdf5','r')\n",
    "for x in h:\n",
    "    print(x[:100])\n",
    "    res = ast.literal_eval(x)\n",
    "    print(x)\n",
    "print(type(res))\n",
    "for k,v in res.items():\n",
    "    print(v)\n",
    "h.close()'''\n",
    "x = dd.io.load('feats.h5')\n",
    "print(type(x['data']))\n",
    "print(x['data'][1][1][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x = np.load('features.npy',allow_pickle=True)\n",
    "print(x.shape)\n",
    "for y in x:\n",
    "    print(y[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from keras.models import Sequential\n",
    "#from keras.layers import Dense\n",
    "#from keras.optimizers import SGD\n",
    "import numpy as np\n",
    "x = np.load('features.npy',allow_pickle=True)\n",
    "\n",
    "'''def define_model(in_shape=(8,2048), out_shape=174):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(1024, activation='relu', kernel_initializer='he_uniform'),input_shape=in_shape)\n",
    "    model.add(Dense(out_shape, activation='softmax'))\n",
    "    opt = SGD(lr=0.01, momentum=0.9)\n",
    "    model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['categorical_accuracy'])\n",
    "    return model\n",
    "\n",
    "model = define_model()'''\n",
    "x = x[:100,]\n",
    "y = [a[0] for a in x]\n",
    "x = [a[1] for a in x]\n",
    "#print(y,x)\n",
    "#history = model.fit_generator(train_it, steps_per_epoch=len(x), epochs=200, verbose=0)\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(16384,2048)\n",
    "        self.output = nn.Linear(2048, 174)\n",
    "        \n",
    "        # Define sigmoid activation and softmax output \n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Pass the input tensor through each of our operations\n",
    "        x = self.hidden(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.output(x)\n",
    "        x = self.softmax(x)\n",
    "        #print(x.size())\n",
    "        \n",
    "        return x\n",
    "net = Network()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    for data in zip(x,y):\n",
    "        inputs, labels = data\n",
    "        labels = np.asarray([0 if x!= labels else 1 for x in range(174)])\n",
    "        #print(labels)\n",
    "        inputs = inputs.reshape(-1)\n",
    "        inputs = Variable(torch.from_numpy(inputs))\n",
    "        labels = Variable(torch.from_numpy(labels))\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        print(outputs.size(),labels.size())\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "print('Finished Training')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
